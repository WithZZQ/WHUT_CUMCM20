{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from string import punctuation\n",
    "from sklearn import svm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "from itertools import chain\n",
    "from wordcloud import WordCloud\n",
    "font = FontProperties(fname=r\"c:\\windows\\fonts\\simsun.ttc\", size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据导入\n",
    "file_path = './Problem_C_Data/hair_dryer.tsv'\n",
    "df = pd.read_csv(file_path, sep='\\t', header=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Helpful %'] = np.where(df['helpful_votes'] > 0, df['helpful_votes'] / df['total_votes'], -1)\n",
    "df['% Upvote'] = pd.cut(df['Helpful %'], bins = [-1, 0, 0.2, 0.4, 0.6, 0.8, 1.0], labels = ['Empty', '0-20%', '20-40%', '40-60%', '60-80%', '80-100%'], include_lowest = True)\n",
    "df.head()\n",
    "\n",
    "df_s = df.groupby(['star_rating', '% Upvote']).agg({'review_id': 'count'})\n",
    "df_s = df_s.unstack()\n",
    "df_s.columns = df_s.columns.get_level_values(1)\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "\n",
    "sns.heatmap(df_s[df_s.columns[::-1]].T, cmap = 'YlGnBu', linewidths=.5, annot = True)\n",
    "plt.yticks(rotation=0)\n",
    "plt.title('How helpful users find among user scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保留不是3的评论\n",
    "filtered_data = df[df['star_rating'] != 3]\n",
    "def partition(x):\n",
    "    if x>3:\n",
    "        return 'positive'\n",
    "    return 'negative'\n",
    "\n",
    "actual_score = filtered_data['star_rating']\n",
    "positiveNegative = actual_score.map(partition)\n",
    "filtered_data['Score'] = positiveNegative\n",
    "filtered_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Sorting data points according to the 'ProductId'\n",
    "sorted_data = filtered_data.sort_values('product_id', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\n",
    "\n",
    "# Eliminating the duplicate data points based on: 'UserId', 'ProfileName', 'Time', 'Summary'\n",
    "final = sorted_data.drop_duplicates(subset={'review_id', 'product_title', 'review_date', 'review_body'}, keep='first', inplace=False)\n",
    "\n",
    "# Eliminating the row where 'HelpfulnessDenominator' is greater than 'HelpfulnessNumerator' as these are the wrong entry\n",
    "final = final[final['total_votes'] >= final['helpful_votes']]\n",
    "\n",
    "# Getting shape of final data frame\n",
    "display(final.shape)\n",
    "\n",
    "\n",
    "# Creating the set of stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "# For stemming purpose\n",
    "snow = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "# Defining function to clean html tags\n",
    "def cleanhtml(sentence):\n",
    "    cleaner = re.compile('<.*>')\n",
    "    cleantext = re.sub(cleaner, ' ', sentence)\n",
    "    return cleantext\n",
    "\n",
    "# Defining function to remove special symbols\n",
    "def cleanpunc(sentence):\n",
    "    cleaned = re.sub(r'[?|.|!|*|@|#|\\'|\"|,|)|(|\\|/]', r'', sentence)\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "# Important steps to clean the text data. Please trace it out carefully\n",
    "i = 0\n",
    "str1 = ''\n",
    "all_positive_words = []\n",
    "all_negative_words = []\n",
    "final_string = []\n",
    "s=''\n",
    "for sent in final['review_body'].values:\n",
    "    filtered_sentence = []\n",
    "    sent = cleanhtml(sent)\n",
    "    for w in sent.split():\n",
    "        for cleaned_words in cleanpunc(w).split():\n",
    "            if ((cleaned_words.isalpha()) & (len(cleaned_words)>2)):\n",
    "                if (cleaned_words.lower() not in stop):\n",
    "                    s = (snow.stem(cleaned_words.lower())).encode('utf-8')\n",
    "                    filtered_sentence.append(s)\n",
    "                    if (final['Score'].values)[i] == 'positive':\n",
    "                        all_positive_words.append(s)\n",
    "                    if (final['Score'].values)[i] == 'negative':\n",
    "                        all_negative_words.append(s)\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "    str1 = b\" \".join(filtered_sentence)\n",
    "    final_string.append(str1)\n",
    "    i += 1\n",
    "    \n",
    "# Adding new column into dataframe to store cleaned text\n",
    "final['CleanedText'] = final_string\n",
    "final['CleanedText'] = final['CleanedText'].str.decode('utf-8')\n",
    "\n",
    "\n",
    "# Getting shape of new datset\n",
    "print(final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = final['CleanedText']\n",
    "y_dict = {1:0, 2:0, 4:1, 5:1}\n",
    "y = final['star_rating'].map(y_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c = CountVectorizer(stop_words = 'english')\n",
    "\n",
    "def text_fit(X, y, model,clf_model,coef_show=1):\n",
    "    \n",
    "    X_c = model.fit_transform(X)\n",
    "    print('# features: {}'.format(X_c.shape[1]))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_c, y, random_state=0)\n",
    "    print('# train records: {}'.format(X_train.shape[0]))\n",
    "    print('# test records: {}'.format(X_test.shape[0]))\n",
    "    clf = clf_model.fit(X_train, y_train)\n",
    "    acc = clf.score(X_test, y_test)\n",
    "    print ('Model Accuracy: {}'.format(acc))\n",
    "    \n",
    "    if coef_show == 1: \n",
    "        w = model.get_feature_names()\n",
    "        coef = clf.coef_.tolist()[0]\n",
    "        coeff_df = pd.DataFrame({'Word' : w, 'Coefficient' : coef})\n",
    "        coeff_df = coeff_df.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\n",
    "        print('')\n",
    "        print('-Top 20 positive-')\n",
    "        print(coeff_df.head(20).to_string(index=False))\n",
    "        print('')\n",
    "        print('-Top 20 negative-')        \n",
    "        print(coeff_df.tail(20).to_string(index=False))\n",
    "    \n",
    "    \n",
    "text_fit(X, y, c, LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['star_rating'] == 5]\n",
    "df = df[df['% Upvote'].isin(['0-20%', '20-40%', '60-80%', '80-100%'])]\n",
    "df.shape\n",
    "\n",
    "X = df['review_body']\n",
    "y_dict = {'0-20%': 0, '20-40%': 0, '60-80%': 1, '80-100%': 1}\n",
    "y = df['% Upvote'].map(y_dict)\n",
    "\n",
    "print('Class distribution:')\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s = pd.DataFrame(data = [X,y]).T\n",
    "\n",
    "Downvote_records = len(df_s[df_s['% Upvote'] == 0])\n",
    "Downvote_indices = np.array(df_s[df_s['% Upvote'] == 0].index)\n",
    "\n",
    "Upvote_indices = df_s[df_s['% Upvote'] == 1].index\n",
    "\n",
    "random_upvote_indices = np.random.choice(Upvote_indices, Downvote_records, replace = False)\n",
    "random_upvote_indices = np.array(random_upvote_indices)\n",
    "\n",
    "under_sample_indices = np.concatenate([Downvote_indices,random_upvote_indices])\n",
    "\n",
    "under_sample_data = df_s.ix[under_sample_indices, :]\n",
    "X_u = under_sample_data['review_body']\n",
    "under_sample_data['% Upvote'] = under_sample_data['% Upvote'].astype(int)\n",
    "y_u = under_sample_data['% Upvote']\n",
    "\n",
    "\n",
    "print(\"Percentage of upvote transactions: \", len(under_sample_data[under_sample_data['% Upvote'] == 1])/len(under_sample_data))\n",
    "print(\"Percentage of downvote transactions: \", len(under_sample_data[under_sample_data['% Upvote'] == 0])/len(under_sample_data))\n",
    "print(\"Total number of records in resampled data: \", len(under_sample_data))\n",
    "\n",
    "#pd.set_option('display.max_colwidth', -1)\n",
    "print('Downvote score 5 comments examples:')\n",
    "print(under_sample_data[under_sample_data['% Upvote']==0]['review_body'].iloc[:100:20])\n",
    "print('Upvote score 5 comments examples')\n",
    "print(under_sample_data[under_sample_data['% Upvote']==1]['review_body'].iloc[:100:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "under_sample_data['word_count'] = under_sample_data['review_body'].apply(lambda x: len(x.split()))\n",
    "under_sample_data['capital_count'] = under_sample_data['review_body'].apply(lambda x: sum(1 for c in x if c.isupper()))\n",
    "under_sample_data['question_mark'] = under_sample_data['review_body'].apply(lambda x: sum(1 for c in x if c == '?'))\n",
    "under_sample_data['exclamation_mark'] = under_sample_data['review_body'].apply(lambda x: sum(1 for c in x if c == '!'))\n",
    "under_sample_data['punctuation'] = under_sample_data['review_body'].apply(lambda x: sum(1 for c in x if c in punctuation))\n",
    "\n",
    "print(under_sample_data.groupby('% Upvote').agg({'word_count': 'mean', 'capital_count': 'mean', 'question_mark': 'mean', 'exclamation_mark': 'mean', 'punctuation': 'mean'}).T)\n",
    "\n",
    "X_num = under_sample_data[under_sample_data.columns.difference(['% Upvote', 'Text'])]\n",
    "y_num = under_sample_data['% Upvote']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user = df.groupby(['customer_id']).agg({'star_rating':['count', 'mean']})\n",
    "df_user.columns = df_user.columns.get_level_values(1)\n",
    "df_user.columns = ['Score count', 'Score mean']\n",
    "df_user = df_user.sort_values(by = 'Score count', ascending = False)\n",
    "print(df_user.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_token_adj(score, benchmark, odf,userid='all'):\n",
    "    \n",
    "    if userid != 'all':\n",
    "        df = odf[(odf['customer_id'] == userid) & (odf['star_rating'] == score)]['review_body']\n",
    "    else:\n",
    "        df = odf[odf['star_rating'] == score]['review_body']\n",
    "        \n",
    "    count = len(df)\n",
    "    total_text = ' '.join(df)\n",
    "    total_text = total_text.lower()\n",
    "    stop = set(stopwords.words('english'))\n",
    "    total_text = nltk.word_tokenize(total_text)\n",
    "    total_text = [word for word in total_text if word not in stop and len(word) >= 3]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    total_text = [lemmatizer.lemmatize(w,'a') for w in total_text]\n",
    "    # get adjective only\n",
    "    total_text = [word for word, form in nltk.pos_tag(total_text) if form == 'JJ']\n",
    "    \n",
    "    text = nltk.Text(total_text)\n",
    "    fdist = nltk.FreqDist(text)\n",
    "    \n",
    "    # return only phrase occurs more than benchmark of his reviews\n",
    "    return sorted([(w,fdist[w],str(round(fdist[w]/count*100,2))+'%') for w in set(text) if fdist[w] >= count*benchmark], key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# score 1-5 reviews with all users\n",
    "index = ['Phrase', 'Count', 'Occur %']\n",
    "\n",
    "for j in range(1,6):\n",
    "    test = pd.DataFrame()\n",
    "    d = get_token_adj(j, 0.05,df)\n",
    "    print('score {} reviews most popular adjectives word:'.format(j))\n",
    "    for i in d:\n",
    "        test = test.append(pd.Series(i, index = index), ignore_index = True)\n",
    "    test = test.sort_values('Count', ascending=False)\n",
    "    print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
